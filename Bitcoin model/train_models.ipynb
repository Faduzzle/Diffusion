{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e270652e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thoma\\anaconda3\\envs\\MachineLearning\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from trainer import train_model_from_config  # your main train launcher\n",
    "from config import CONFIG                         # your configuration\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcf33255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "history_len: 50\n",
      "predict_len: 20\n",
      "model_dim: 256\n",
      "n_epochs: 500\n",
      "samples_per_epoch: 700\n",
      "batch_size: 64\n",
      "lr: 0.001\n",
      "ema_decay: 0.999\n",
      "checkpoint_freq: 50\n",
      "checkpoint_dir: checkpoints\n",
      "save_name: diffusion_model\n",
      "train_data_path: C:\\Users\\thoma\\Desktop\\Diffusion\\Bitcoin model\\training data\n",
      "test_data_path: C:\\Users\\thoma\\Desktop\\Diffusion\\Bitcoin model\\Testing Data\n",
      "mask_prob: 0.01\n",
      "device: cuda\n",
      "checkpoint_path: C:\\Users\\thoma\\Desktop\\Diffusion\\Bitcoin model\\checkpoints\\diffusion_model.pth\n",
      "num_diffusion_timesteps: 500\n",
      "num_paths: 800\n",
      "cond_drop_prob: 0.1\n",
      "classifier_free_guidance_weight: 2.0\n"
     ]
    }
   ],
   "source": [
    "# Check training settings\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b2014f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ•Ô∏è Using device: cuda\n",
      "üîé Before dropna: (4844, 2)\n",
      "üîé After dropna: (4844, 2)\n",
      "üß™ Loaded train_tensor shape: torch.Size([4844, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|‚ñà‚ñà‚ñà‚ñâ                                   | 51/500 [05:43<50:20,  6.73s/it, loss=0.0183]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_model_from_config()\n",
      "File \u001b[1;32mc:\\Users\\thoma\\Desktop\\Diffusion\\Bitcoin model\\trainer.py:106\u001b[0m, in \u001b[0;36mtrain_model_from_config\u001b[1;34m()\u001b[0m\n\u001b[0;32m     98\u001b[0m model \u001b[38;5;241m=\u001b[39m ScoreTransformerNet(\n\u001b[0;32m     99\u001b[0m     input_dim\u001b[38;5;241m=\u001b[39mtrain_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m    100\u001b[0m     history_len\u001b[38;5;241m=\u001b[39mhistory_len,\n\u001b[0;32m    101\u001b[0m     predict_len\u001b[38;5;241m=\u001b[39mpredict_len,\n\u001b[0;32m    102\u001b[0m     model_dim\u001b[38;5;241m=\u001b[39mCONFIG\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m256\u001b[39m)\n\u001b[0;32m    103\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    105\u001b[0m sde \u001b[38;5;241m=\u001b[39m VPSDE()\n\u001b[1;32m--> 106\u001b[0m train(model, sde, dataset,\n\u001b[0;32m    107\u001b[0m       history_len\u001b[38;5;241m=\u001b[39mhistory_len,\n\u001b[0;32m    108\u001b[0m       predict_len\u001b[38;5;241m=\u001b[39mpredict_len,\n\u001b[0;32m    109\u001b[0m       n_epochs\u001b[38;5;241m=\u001b[39mCONFIG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    110\u001b[0m       batch_size\u001b[38;5;241m=\u001b[39mCONFIG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    111\u001b[0m       lr\u001b[38;5;241m=\u001b[39mCONFIG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    112\u001b[0m       save_dir\u001b[38;5;241m=\u001b[39mCONFIG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    113\u001b[0m       checkpoint_freq\u001b[38;5;241m=\u001b[39mCONFIG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint_freq\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    114\u001b[0m       save_name\u001b[38;5;241m=\u001b[39mCONFIG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msave_name\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    115\u001b[0m       device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\thoma\\Desktop\\Diffusion\\Bitcoin model\\trainer.py:51\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, sde, dataset, history_len, predict_len, n_epochs, batch_size, lr, save_dir, checkpoint_freq, device, save_name, ema_decay)\u001b[0m\n\u001b[0;32m     48\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean((std \u001b[38;5;241m*\u001b[39m score_pred \u001b[38;5;241m+\u001b[39m noise) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     50\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 51\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     52\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     53\u001b[0m update_ema(ema_model, model, ema_decay)\n",
      "File \u001b[1;32mc:\\Users\\thoma\\anaconda3\\envs\\MachineLearning\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    628\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\thoma\\anaconda3\\envs\\MachineLearning\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[0;32m    348\u001b[0m     tensors,\n\u001b[0;32m    349\u001b[0m     grad_tensors_,\n\u001b[0;32m    350\u001b[0m     retain_graph,\n\u001b[0;32m    351\u001b[0m     create_graph,\n\u001b[0;32m    352\u001b[0m     inputs,\n\u001b[0;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    355\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\thoma\\anaconda3\\envs\\MachineLearning\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model_from_config()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
