{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier-Free Guidance for Bitcoin Time Series Diffusion\n",
    "\n",
    "This notebook demonstrates how classifier-free guidance (CFG) works in the context of time series prediction with diffusion models.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "**Classifier-Free Guidance** allows us to control the strength of conditioning on historical data:\n",
    "- `guidance_scale = 1.0`: Standard conditional generation\n",
    "- `guidance_scale > 1.0`: Stronger conditioning on history\n",
    "- `guidance_scale < 1.0`: Weaker conditioning (more unconditional)\n",
    "\n",
    "The formula is: `score = uncond_score + guidance_scale * (cond_score - uncond_score)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from src.evaluation import DiffusionPredictor, MetricCalculator\n",
    "from src.data import DataPreprocessor\n",
    "from src.utils import load_config, plot_predictions\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Trained Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = load_config('../configs/bitcoin_cfg_experiment.yaml')\n",
    "\n",
    "# Load trained model (adjust path as needed)\n",
    "model_path = '../models/checkpoints_cfg/best_model.pt'\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"Model not found at {model_path}\")\n",
    "    print(\"Please train a model first using: python scripts/train.py --config configs/bitcoin_cfg_experiment.yaml\")\n",
    "else:\n",
    "    predictor = DiffusionPredictor.from_checkpoint(model_path)\n",
    "    print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_data_path = '../data/processed/bitcoin_test.pt'\n",
    "if os.path.exists(test_data_path):\n",
    "    preprocessor = DataPreprocessor({})\n",
    "    test_data, metadata = preprocessor.load_processed_data(test_data_path)\n",
    "    print(f\"Test data shape: {test_data.shape}\")\n",
    "else:\n",
    "    print(\"Test data not found. Please run data preparation first.\")\n",
    "    print(\"python scripts/prepare_data.py --input data/raw/bitcoin.csv --output data/processed/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a sample for demonstration\n",
    "history_len = config['model']['history_len']\n",
    "predict_len = config['model']['predict_len']\n",
    "\n",
    "# Use the middle portion of test data\n",
    "start_idx = len(test_data) // 2\n",
    "sample_history = test_data[start_idx:start_idx + history_len].unsqueeze(0)\n",
    "sample_future = test_data[start_idx + history_len:start_idx + history_len + predict_len]\n",
    "\n",
    "print(f\"Sample history shape: {sample_history.shape}\")\n",
    "print(f\"Sample future shape: {sample_future.shape}\")\n",
    "print(f\"Ground truth available: {len(sample_future) == predict_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Different Guidance Scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different guidance scales\n",
    "guidance_scales = [0.5, 1.0, 1.5, 2.0, 3.0, 5.0]\n",
    "num_samples = 50  # Fewer samples for speed\n",
    "num_steps = 500   # Fewer steps for speed\n",
    "\n",
    "predictions = {}\n",
    "\n",
    "for scale in guidance_scales:\n",
    "    print(f\"Generating predictions with guidance scale {scale}...\")\n",
    "    \n",
    "    pred = predictor.predict(\n",
    "        history=sample_history,\n",
    "        num_samples=num_samples,\n",
    "        num_steps=num_steps,\n",
    "        guidance_scale=scale,\n",
    "        denormalize=True,\n",
    "        return_dict=True\n",
    "    )\n",
    "    \n",
    "    predictions[scale] = pred\n",
    "    print(f\"  Mean prediction range: [{pred['mean'].min():.4f}, {pred['mean'].max():.4f}]\")\n",
    "    print(f\"  Prediction std: {pred['std'].mean():.4f}\")\n",
    "\n",
    "print(\"\\nPrediction generation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Effects of Classifier-Free Guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive comparison plot\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Denormalize history and ground truth if available\n",
    "history_denorm = sample_history.squeeze().numpy()\n",
    "if predictor.preprocessor is not None:\n",
    "    history_denorm = predictor.preprocessor.denormalize(sample_history.squeeze()).numpy()\n",
    "    if len(sample_future) == predict_len:\n",
    "        future_denorm = predictor.preprocessor.denormalize(sample_future).numpy()\n",
    "    else:\n",
    "        future_denorm = None\n",
    "else:\n",
    "    future_denorm = sample_future.numpy() if len(sample_future) == predict_len else None\n",
    "\n",
    "for i, scale in enumerate(guidance_scales):\n",
    "    ax = axes[i]\n",
    "    pred = predictions[scale]\n",
    "    \n",
    "    # Time indices\n",
    "    history_time = np.arange(-history_len, 0)\n",
    "    future_time = np.arange(predict_len)\n",
    "    \n",
    "    # Plot history\n",
    "    ax.plot(history_time, history_denorm, 'k-', label='History', linewidth=2, alpha=0.7)\n",
    "    \n",
    "    # Plot mean prediction\n",
    "    ax.plot(future_time, pred['mean'][0], 'b-', label='Mean Prediction', linewidth=2)\n",
    "    \n",
    "    # Plot confidence intervals\n",
    "    if 'quantiles' in pred:\n",
    "        ax.fill_between(\n",
    "            future_time,\n",
    "            pred['quantiles'][0.1][0],\n",
    "            pred['quantiles'][0.9][0],\n",
    "            alpha=0.2, color='blue', label='80% CI'\n",
    "        )\n",
    "        ax.fill_between(\n",
    "            future_time,\n",
    "            pred['quantiles'][0.25][0],\n",
    "            pred['quantiles'][0.75][0],\n",
    "            alpha=0.3, color='blue', label='50% CI'\n",
    "        )\n",
    "    \n",
    "    # Plot ground truth if available\n",
    "    if future_denorm is not None:\n",
    "        ax.plot(future_time, future_denorm, 'r-', label='Ground Truth', linewidth=2)\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_title(f'Guidance Scale = {scale}', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Time Steps')\n",
    "    ax.set_ylabel('Log Returns')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    if i == 0:\n",
    "        ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Effect of Classifier-Free Guidance on Bitcoin Predictions', \n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Prediction Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare prediction uncertainty across guidance scales\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot 1: Mean prediction variance\n",
    "plt.subplot(2, 2, 1)\n",
    "variances = [predictions[scale]['std'][0].mean() for scale in guidance_scales]\n",
    "plt.plot(guidance_scales, variances, 'o-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Guidance Scale')\n",
    "plt.ylabel('Mean Prediction Std')\n",
    "plt.title('Prediction Uncertainty vs Guidance Scale')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Prediction spread (max - min)\n",
    "plt.subplot(2, 2, 2)\n",
    "spreads = [predictions[scale]['mean'][0].max() - predictions[scale]['mean'][0].min() \n",
    "           for scale in guidance_scales]\n",
    "plt.plot(guidance_scales, spreads, 'o-', linewidth=2, markersize=8, color='orange')\n",
    "plt.xlabel('Guidance Scale')\n",
    "plt.ylabel('Prediction Range')\n",
    "plt.title('Prediction Range vs Guidance Scale')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Distribution of final predictions\n",
    "plt.subplot(2, 2, 3)\n",
    "final_predictions = [predictions[scale]['samples'][0, :, -1] for scale in guidance_scales]\n",
    "plt.boxplot(final_predictions, labels=[f'{s}' for s in guidance_scales])\n",
    "plt.xlabel('Guidance Scale')\n",
    "plt.ylabel('Final Step Prediction')\n",
    "plt.title('Distribution of Final Predictions')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Quantile width (uncertainty measure)\n",
    "plt.subplot(2, 2, 4)\n",
    "quantile_widths = []\n",
    "for scale in guidance_scales:\n",
    "    if 'quantiles' in predictions[scale]:\n",
    "        width = (predictions[scale]['quantiles'][0.9][0] - \n",
    "                predictions[scale]['quantiles'][0.1][0]).mean()\n",
    "        quantile_widths.append(width)\n",
    "    else:\n",
    "        quantile_widths.append(0)\n",
    "\n",
    "plt.plot(guidance_scales, quantile_widths, 'o-', linewidth=2, markersize=8, color='green')\n",
    "plt.xlabel('Guidance Scale')\n",
    "plt.ylabel('80% Quantile Width')\n",
    "plt.title('Uncertainty Band Width vs Guidance Scale')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate predictions quantitatively (if ground truth is available)\n",
    "if future_denorm is not None:\n",
    "    metric_calc = MetricCalculator()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for scale in guidance_scales:\n",
    "        pred = predictions[scale]\n",
    "        metrics = metric_calc.compute_all_metrics(pred, future_denorm)\n",
    "        \n",
    "        result = {'guidance_scale': scale}\n",
    "        result.update(metrics)\n",
    "        results.append(result)\n",
    "    \n",
    "    # Convert to DataFrame for easy viewing\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(\"\\nQuantitative Evaluation Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(results_df.round(4))\n",
    "    \n",
    "    # Plot key metrics\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # MSE\n",
    "    axes[0].plot(results_df['guidance_scale'], results_df['mse'], 'o-', linewidth=2)\n",
    "    axes[0].set_xlabel('Guidance Scale')\n",
    "    axes[0].set_ylabel('MSE')\n",
    "    axes[0].set_title('Mean Squared Error')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Directional Accuracy\n",
    "    axes[1].plot(results_df['guidance_scale'], results_df['directional_accuracy'], 'o-', linewidth=2, color='orange')\n",
    "    axes[1].set_xlabel('Guidance Scale')\n",
    "    axes[1].set_ylabel('Directional Accuracy')\n",
    "    axes[1].set_title('Directional Accuracy')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # CRPS (if available)\n",
    "    if 'crps' in results_df.columns:\n",
    "        axes[2].plot(results_df['guidance_scale'], results_df['crps'], 'o-', linewidth=2, color='green')\n",
    "        axes[2].set_xlabel('Guidance Scale')\n",
    "        axes[2].set_ylabel('CRPS')\n",
    "        axes[2].set_title('Continuous Ranked Probability Score')\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find optimal guidance scale\n",
    "    optimal_mse_scale = results_df.loc[results_df['mse'].idxmin(), 'guidance_scale']\n",
    "    optimal_dir_scale = results_df.loc[results_df['directional_accuracy'].idxmax(), 'guidance_scale']\n",
    "    \n",
    "    print(f\"\\nOptimal guidance scales:\")\n",
    "    print(f\"  Best MSE: {optimal_mse_scale}\")\n",
    "    print(f\"  Best Directional Accuracy: {optimal_dir_scale}\")\n",
    "    \n",
    "else:\n",
    "    print(\"Ground truth not available for quantitative evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights\n",
    "\n",
    "From this analysis, you should observe:\n",
    "\n",
    "1. **Guidance Scale Effects**:\n",
    "   - `guidance_scale < 1.0`: More unconditional, potentially more diverse but less conditioned on history\n",
    "   - `guidance_scale = 1.0`: Standard conditional generation\n",
    "   - `guidance_scale > 1.0`: Stronger conditioning on historical patterns\n",
    "\n",
    "2. **Trade-offs**:\n",
    "   - Higher guidance → More deterministic, potentially better conditioning\n",
    "   - Lower guidance → More uncertainty, potentially more diverse samples\n",
    "   - Optimal scale depends on your specific use case\n",
    "\n",
    "3. **Practical Recommendations**:\n",
    "   - For most applications: `guidance_scale = 1.5 to 2.5`\n",
    "   - For high-confidence predictions: `guidance_scale = 3.0+`\n",
    "   - For diverse scenario generation: `guidance_scale = 0.5 to 1.0`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: Conditional vs Unconditional Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare conditional vs unconditional generation directly\n",
    "print(\"Generating conditional vs unconditional samples...\")\n",
    "\n",
    "# Generate with strong conditioning\n",
    "conditional_pred = predictor.predict(\n",
    "    history=sample_history,\n",
    "    num_samples=num_samples,\n",
    "    num_steps=num_steps,\n",
    "    guidance_scale=3.0,  # Strong conditioning\n",
    "    denormalize=True,\n",
    "    return_dict=True\n",
    ")\n",
    "\n",
    "# Generate with weak conditioning (more unconditional)\n",
    "unconditional_pred = predictor.predict(\n",
    "    history=sample_history,\n",
    "    num_samples=num_samples,\n",
    "    num_steps=num_steps,\n",
    "    guidance_scale=0.5,  # Weak conditioning\n",
    "    denormalize=True,\n",
    "    return_dict=True\n",
    ")\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Time indices\n",
    "history_time = np.arange(-history_len, 0)\n",
    "future_time = np.arange(predict_len)\n",
    "\n",
    "for i, (pred, title, scale) in enumerate([\n",
    "    (unconditional_pred, 'Weak Conditioning (Scale=0.5)', 0.5),\n",
    "    (conditional_pred, 'Strong Conditioning (Scale=3.0)', 3.0)\n",
    "]):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Plot history\n",
    "    ax.plot(history_time, history_denorm, 'k-', label='History', linewidth=2)\n",
    "    \n",
    "    # Plot multiple sample trajectories\n",
    "    n_show = min(10, num_samples)\n",
    "    for j in range(n_show):\n",
    "        ax.plot(future_time, pred['samples'][j, 0], \n",
    "               alpha=0.3, color='blue', linewidth=1)\n",
    "    \n",
    "    # Plot mean\n",
    "    ax.plot(future_time, pred['mean'][0], 'b-', \n",
    "           label='Mean Prediction', linewidth=3)\n",
    "    \n",
    "    # Plot ground truth if available\n",
    "    if future_denorm is not None:\n",
    "        ax.plot(future_time, future_denorm, 'r-', \n",
    "               label='Ground Truth', linewidth=2)\n",
    "    \n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Time Steps')\n",
    "    ax.set_ylabel('Log Returns')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nUnconditional (scale=0.5) - Sample std: {unconditional_pred['std'][0].mean():.4f}\")\n",
    "print(f\"Conditional (scale=3.0) - Sample std: {conditional_pred['std'][0].mean():.4f}\")\n",
    "print(f\"Std ratio (uncond/cond): {unconditional_pred['std'][0].mean() / conditional_pred['std'][0].mean():.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}