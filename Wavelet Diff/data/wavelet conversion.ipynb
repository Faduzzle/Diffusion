{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precomputing time series windows into wavelet frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶️  Running wavelet conversion in **TEST** mode\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ─── 1. Imports & Mode Selection ─────────────────────────────────────────────────\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pywt\n",
    "\n",
    "# ─── Change only this line to either \"train\" or \"test\" ─────────────────────────────\n",
    "mode = \"test\"   # options: \"train\" or \"test\"\n",
    "# mode = \"test\"\n",
    "\n",
    "assert mode in (\"train\", \"test\"), \"mode must be 'train' or 'test'\"\n",
    "\n",
    "print(f\"▶️  Running wavelet conversion in **{mode.upper()}** mode\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➤ Input CSV: C:\\Users\\thoma\\Desktop\\Wavelet Diff\\data\\Testing Data\\bitcoin_2010-07-29_2025-04-25_test.csv\n",
      "➤ Output folder: C:\\Users\\thoma\\Desktop\\Wavelet Diff\\data\\wavelets\\test wavelet\n",
      "➤ Wavelet = db4, level = 4, window_len = 70\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ─── 2. Build Input / Output Paths Based on Mode ─────────────────────────────────\n",
    "\n",
    "# Base folders\n",
    "data_folder = r\"C:\\Users\\thoma\\Desktop\\Wavelet Diff\\data\"\n",
    "wavelets_base = os.path.join(data_folder, \"wavelets\")\n",
    "\n",
    "# Input CSVs (they already exist under Training Data / Testing Data)\n",
    "input_csv = os.path.join(\n",
    "    data_folder,\n",
    "    \"Training Data\"   if mode == \"train\" else \"Testing Data\",\n",
    "    \"bitcoin_2010-07-29_2025-04-25_\" + mode + \".csv\"\n",
    ")\n",
    "\n",
    "# Output folder for this mode (“train wavelet” or “test wavelet”)\n",
    "output_dir = os.path.join(\n",
    "    wavelets_base,\n",
    "    \"train wavelet\" if mode == \"train\" else \"test wavelet\"\n",
    ")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Wavelet and window parameters (shared)\n",
    "wavelet_name = \"db4\"   # change as needed (e.g. \"sym6\", \"coif3\", etc.)\n",
    "level = 4              # decomposition level\n",
    "window_len = 70        # each window spans 70 days\n",
    "step = 1               # sliding step\n",
    "\n",
    "print(\"➤ Input CSV:\", input_csv)\n",
    "print(\"➤ Output folder:\", output_dir)\n",
    "print(f\"➤ Wavelet = {wavelet_name}, level = {level}, window_len = {window_len}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded TEST: 1077 days × 1 feature (Close)\n",
      "✅ Loaded dates dtype = datetime64[ns], length = 1077\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ─── 3. Load CSV & Extract “Close” + “Date” ──────────────────────────────────────\n",
    "#\n",
    "# CSV columns: Date (YYYY-MM-DD), Year, Month, Day, Close.\n",
    "full_df = pd.read_csv(input_csv, header=0, parse_dates=[\"Date\"])\n",
    "\n",
    "# Ensure required columns exist\n",
    "for col in (\"Date\", \"Close\"):\n",
    "    if col not in full_df.columns:\n",
    "        raise KeyError(f\"CSV must contain a '{col}' column.\")\n",
    "\n",
    "# 1. “Close” → float32 series of shape (num_days,)\n",
    "series = full_df[\"Close\"].astype(np.float32).to_numpy()\n",
    "\n",
    "# 2. “Date” → datetime64[ns] array of shape (num_days,)\n",
    "dates = full_df[\"Date\"].to_numpy()\n",
    "\n",
    "# Reshape series to (num_days, 1) so num_features = 1\n",
    "full_data = series.reshape(-1, 1)\n",
    "num_days, num_features = full_data.shape  # num_features will be 1\n",
    "\n",
    "print(f\"✅ Loaded {mode.upper()}: {num_days} days × {num_features} feature (Close)\")\n",
    "print(f\"✅ Loaded dates dtype = {dates.dtype}, length = {len(dates)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶️ Series length = 1077. Padding 11 samples to reach multiple of 16 → 1088.\n",
      "\n",
      "▶️ After padding: N_pad = 1088.  N_pad % 16 == 0 (should be 0).\n",
      ">>> series_padded type: <class 'numpy.ndarray'>\n",
      ">>> series_padded dtype/shape: float32 (1088,)\n",
      "▶️ pywt.swt_max_level(N_pad=1088) = 6\n",
      "\n",
      "▶️ Proceeding with SWT at level = 4 on length = 1088.\n",
      "\n",
      "▶️ Raw SWT output arrays:\n",
      "   cA_4 shape = (1088,)\n",
      "   cD_4 shape = (1088,)\n",
      "   cD_3 shape = (1088,)\n",
      "   cD_2 shape = (1088,)\n",
      "   cD_1 shape = (1088,)\n",
      "\n",
      "✅ Completed SWT (padded) at level = 4\n",
      "   • coeffs_cA4 shape = (1088, 1)\n",
      "   • coeffs_cD4 shape = (1088, 1)\n",
      "   • coeffs_cD3 shape = (1088, 1)\n",
      "   • coeffs_cD2 shape = (1088, 1)\n",
      "   • coeffs_cD1 shape = (1088, 1) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ─── 4. Compute SWT at Level 4 with Padding (fixed unpacking) ─────────────────────\n",
    "import pywt\n",
    "\n",
    "# 1) Original series length\n",
    "N = len(series)  # e.g. 4844\n",
    "\n",
    "# 2) Compute how many to pad so that (N + pad_len) % 16 == 0 for L = 4\n",
    "pad_len = (-N) % 16\n",
    "if pad_len > 0:\n",
    "    print(f\"▶️ Series length = {N}. Padding {pad_len} samples to reach multiple of 16 → {N + pad_len}.\\n\")\n",
    "    series_padded = np.pad(series, (0, pad_len), mode=\"symmetric\")\n",
    "    dates_padded  = np.pad(dates,  (0, pad_len), mode=\"edge\")\n",
    "else:\n",
    "    print(f\"▶️ Series length = {N} is already a multiple of 16.\\n\")\n",
    "    series_padded = series.copy()\n",
    "    dates_padded  = dates.copy()\n",
    "\n",
    "N_pad = len(series_padded)\n",
    "print(f\"▶️ After padding: N_pad = {N_pad}.  N_pad % 16 == {N_pad % 16} (should be 0).\")\n",
    "\n",
    "# 3) Debug: confirm series_padded is a 1D NumPy array of length N_pad\n",
    "print(\">>> series_padded type:\", type(series_padded))\n",
    "print(\">>> series_padded dtype/shape:\", series_padded.dtype, series_padded.shape)\n",
    "if not isinstance(series_padded, np.ndarray) or series_padded.ndim != 1:\n",
    "    raise ValueError(\"series_padded must be a 1D NumPy array before calling pywt.swt!\")\n",
    "\n",
    "# 4) Check max SWT level on the padded length\n",
    "max_level = pywt.swt_max_level(N_pad)\n",
    "print(f\"▶️ pywt.swt_max_level(N_pad={N_pad}) = {max_level}\\n\")\n",
    "\n",
    "# 5) Decide effective_level = min(4, max_level)\n",
    "effective_level = min(4, max_level)\n",
    "if effective_level < 4:\n",
    "    print(f\"⚠️ Even after padding, max_level = {max_level} < 4.  Using level = {effective_level}.\\n\")\n",
    "else:\n",
    "    print(f\"▶️ Proceeding with SWT at level = 4 on length = {N_pad}.\\n\")\n",
    "\n",
    "# 6) Compute SWT at that effective_level\n",
    "#    Now swt_list is a list of length (effective_level + 1)\n",
    "swt_list = pywt.swt(series_padded, wavelet=wavelet_name, level=effective_level, trim_approx=True)\n",
    "\n",
    "# 7) Debug: print out the raw shapes of each coefficient array\n",
    "#    swt_list[0] = cA_L,   swt_list[1] = cD_L,   swt_list[2] = cD_{L-1}, …, swt_list[L] = cD_1\n",
    "print(\"▶️ Raw SWT output arrays:\")\n",
    "for i, coeff_array in enumerate(swt_list):\n",
    "    if i == 0:\n",
    "        print(f\"   cA_{effective_level} shape = {coeff_array.shape}\")\n",
    "    else:\n",
    "        level_num = effective_level - (i - 1)\n",
    "        print(f\"   cD_{level_num} shape = {coeff_array.shape}\")\n",
    "print()\n",
    "\n",
    "# 8) Now explicitly pull out cD1 … cD4 and cA4, padding with zeros if missing:\n",
    "#    (We expect swt_list to be ordered [cA4, cD4, cD3, cD2, cD1] when effective_level=4.)\n",
    "\n",
    "def get_or_zero(arr_list, idx):\n",
    "    \"\"\"Return arr_list[idx] if it exists, else a zero array of shape (N_pad,)\"\"\"\n",
    "    if idx < len(arr_list):\n",
    "        return arr_list[idx]\n",
    "    else:\n",
    "        return np.zeros((N_pad,), dtype=series_padded.dtype)\n",
    "\n",
    "# Since swt_list[0] is always cA_L, and swt_list[1] is cD_L, etc., we can index as follows:\n",
    "cA4_padded = get_or_zero(swt_list, 0)       # index 0 → cA4 (if effective_level=4)\n",
    "cD4_padded = get_or_zero(swt_list, 1)       # index 1 → cD4\n",
    "cD3_padded = get_or_zero(swt_list, 2)       # index 2 → cD3\n",
    "cD2_padded = get_or_zero(swt_list, 3)       # index 3 → cD2\n",
    "cD1_padded = get_or_zero(swt_list, 4)       # index 4 → cD1\n",
    "\n",
    "# 9) Reshape each to (N_pad, 1)\n",
    "coeffs_cA4 = cA4_padded.reshape(-1, 1)\n",
    "coeffs_cD4 = cD4_padded.reshape(-1, 1)\n",
    "coeffs_cD3 = cD3_padded.reshape(-1, 1)\n",
    "coeffs_cD2 = cD2_padded.reshape(-1, 1)\n",
    "coeffs_cD1 = cD1_padded.reshape(-1, 1)\n",
    "\n",
    "print(\"✅ Completed SWT (padded) at level =\", effective_level)\n",
    "print(\"   • coeffs_cA4 shape =\", coeffs_cA4.shape)\n",
    "print(\"   • coeffs_cD4 shape =\", coeffs_cD4.shape)\n",
    "print(\"   • coeffs_cD3 shape =\", coeffs_cD3.shape)\n",
    "print(\"   • coeffs_cD2 shape =\", coeffs_cD2.shape)\n",
    "print(\"   • coeffs_cD1 shape =\", coeffs_cD1.shape, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Stacked TEST windows → features: (1008, 70, 5), dates: (1008, 70)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ─── 5. Slice 70-Day Windows & Collect Corresponding Dates ────────────────────────\n",
    "#\n",
    "# We will slide a 70-day window (step=1) over all num_days. For each start index i:\n",
    "#   1) Extract [i : i + 70] from cD1…cD4 and cA4 → five (70,1) arrays\n",
    "#   2) Concatenate along axis=1 → one (70,5) array\n",
    "#   3) Extract [i : i + 70] from the dates array → one (70,) datetime64[ns] array\n",
    "\n",
    "num_windows = num_days - window_len + 1\n",
    "windowed_feats = []  # to collect (70,5) arrays\n",
    "windowed_dates = []  # to collect (70,) arrays\n",
    "\n",
    "for start in range(0, num_windows, step):\n",
    "    end = start + window_len\n",
    "\n",
    "    # 1) SWT coefficient slices (each shape = (70,1))\n",
    "    slice_d1 = coeffs_cD1[start:end, :]\n",
    "    slice_d2 = coeffs_cD2[start:end, :]\n",
    "    slice_d3 = coeffs_cD3[start:end, :]\n",
    "    slice_d4 = coeffs_cD4[start:end, :]\n",
    "    slice_a4 = coeffs_cA4[start:end, :]\n",
    "\n",
    "    # 2) Concatenate → (70, 5)\n",
    "    feat70x5 = np.concatenate([slice_d1, slice_d2, slice_d3, slice_d4, slice_a4], axis=1)\n",
    "    windowed_feats.append(feat70x5)\n",
    "\n",
    "    # 3) Corresponding dates (70,)\n",
    "    dates70 = dates[start:end]\n",
    "    windowed_dates.append(dates70)\n",
    "\n",
    "# Stack into NumPy arrays\n",
    "all_feats = np.stack(windowed_feats, axis=0)  # shape = (num_windows, 70, 5)\n",
    "all_dates_np = np.stack(windowed_dates, axis=0)  # shape = (num_windows, 70)\n",
    "\n",
    "print(f\"✅ Stacked {mode.upper()} windows → features: {all_feats.shape}, dates: {all_dates_np.shape}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded TRAIN means and stds for TEST normalization (shape: [5,1]):\n",
      "   Band order: [cD1, cD2, cD3, cD4, cA4]\n",
      "   • means = [-6.3051544e-08 -4.2946709e-07 -1.9071674e-06 -9.6872409e-06\n",
      "  1.2477184e-02]\n",
      "   • stds  = [0.05016337 0.05399104 0.05013015 0.05003453 0.05762416]\n",
      "\n",
      "✅ Normalized TEST features → shape = torch.Size([1008, 70, 5])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ─── 6. Z-Score Normalization per Channel ────────────────────────────────────────\n",
    "#\n",
    "# Compute per-channel (level) mean and std on the TRAIN split,\n",
    "# then apply to all_feats. For TEST mode, assume means/stds already saved under train folder.\n",
    "#\n",
    "# Band order in all_feats: [cD1, cD2, cD3, cD4, cA4] (detail → approx)\n",
    "\n",
    "# Convert raw features to tensor for computing stats\n",
    "all_feats_tensor = torch.from_numpy(all_feats.astype(np.float32))  # [num_windows,70,5]\n",
    "\n",
    "if mode == \"train\":\n",
    "    # 1) Compute means and stds across all windows and time-steps for each channel\n",
    "    means = all_feats_tensor.mean(dim=(0, 1))  # shape: (5,) [cD1,cD2,cD3,cD4,cA4]\n",
    "    stds  = all_feats_tensor.std(dim=(0, 1))   # shape: (5,) [cD1,cD2,cD3,cD4,cA4]\n",
    "\n",
    "    # 2) Avoid near-zero std\n",
    "    eps = 1e-6\n",
    "    stds = torch.where(stds < eps, torch.ones_like(stds), stds)\n",
    "\n",
    "    # 3) Save normalization stats under train output_dir\n",
    "    # Reshape to (5,1) for model compatibility and save\n",
    "    means = means.view(5, 1)  # [cD1,cD2,cD3,cD4,cA4] × 1\n",
    "    stds = stds.view(5, 1)    # [cD1,cD2,cD3,cD4,cA4] × 1\n",
    "    \n",
    "    torch.save(means, os.path.join(output_dir, \"wavelet_means.pt\"))\n",
    "    torch.save(stds,  os.path.join(output_dir, \"wavelet_stds.pt\"))\n",
    "\n",
    "    print(\"✅ Computed TRAIN means and stds (shape: [5,1]):\")\n",
    "    print(\"   Band order: [cD1, cD2, cD3, cD4, cA4]\")\n",
    "    print(f\"   • means = {means.squeeze().numpy()}\")\n",
    "    print(f\"   • stds  = {stds.squeeze().numpy()}\\n\")\n",
    "\n",
    "else:  # mode == \"test\"\n",
    "    # 1) Load means and stds from train folder\n",
    "    train_stats_dir = os.path.join(wavelets_base, \"train wavelet\")\n",
    "    means = torch.load(os.path.join(train_stats_dir, \"wavelet_means.pt\"))  # [5,1]\n",
    "    stds  = torch.load(os.path.join(train_stats_dir, \"wavelet_stds.pt\"))   # [5,1]\n",
    "\n",
    "    print(\"✅ Loaded TRAIN means and stds for TEST normalization (shape: [5,1]):\")\n",
    "    print(\"   Band order: [cD1, cD2, cD3, cD4, cA4]\")\n",
    "    print(f\"   • means = {means.squeeze().numpy()}\")\n",
    "    print(f\"   • stds  = {stds.squeeze().numpy()}\\n\")\n",
    "\n",
    "# 4) Normalize features\n",
    "#   train_norm or test_norm: FloatTensor [num_windows, 70, 5]\n",
    "all_feats_norm = (all_feats_tensor - means.view(1, 1, -1)) / stds.view(1, 1, -1)\n",
    "\n",
    "print(f\"✅ Normalized {mode.upper()} features → shape = {all_feats_norm.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved normalized TEST features to:\n",
      "   C:\\Users\\thoma\\Desktop\\Wavelet Diff\\data\\wavelets\\test wavelet\\level4_swt_test_windows_norm.pt\n",
      "✅ Saved TEST dates          to:\n",
      "   C:\\Users\\thoma\\Desktop\\Wavelet Diff\\data\\wavelets\\test wavelet\\level4_test_window_dates.pt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ─── 7. Convert Dates to Int64 & Save Both (Norm. Feats + Dates) ─────────────────\n",
    "#\n",
    "# We will save two .pt files under the mode’s output_dir:\n",
    "#   1) features_norm: FloatTensor of shape [num_windows, 70, 5]\n",
    "#   2) dates        : Int64Tensor of shape [num_windows, 70] (nanoseconds since epoch)\n",
    "\n",
    "# 1) Already have all_feats_norm as FloatTensor\n",
    "# 2) Convert dates to int64 (ns since epoch)\n",
    "all_dates_int = all_dates_np.astype(\"datetime64[ns]\").astype(np.int64)  # shape = (num_windows, 70)\n",
    "all_dates_tensor = torch.from_numpy(all_dates_int)\n",
    "\n",
    "# 3) Save both\n",
    "feats_filename = f\"level4_swt_{mode}_windows_norm.pt\"\n",
    "dates_filename = f\"level4_{mode}_window_dates.pt\"  # same as before\n",
    "\n",
    "torch.save(all_feats_norm, os.path.join(output_dir, feats_filename))\n",
    "torch.save(all_dates_tensor, os.path.join(output_dir, dates_filename))\n",
    "\n",
    "print(f\"✅ Saved normalized {mode.upper()} features to:\\n   {os.path.join(output_dir, feats_filename)}\")\n",
    "print(f\"✅ Saved {mode.upper()} dates          to:\\n   {os.path.join(output_dir, dates_filename)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Verified reload for TEST normalized features:\n",
      "   • features_norm shape = torch.Size([1008, 70, 5])\n",
      "   • dates           shape = torch.Size([1008, 70])\n"
     ]
    }
   ],
   "source": [
    "# ─── 8. Verification ─────────────────────────────────────────────────┐\n",
    "# Reload immediately to confirm integrity                                         │\n",
    "loaded_feats_norm = torch.load(os.path.join(output_dir, feats_filename))\n",
    "loaded_dates      = torch.load(os.path.join(output_dir, dates_filename))\n",
    "\n",
    "assert loaded_feats_norm.shape == all_feats_norm.shape, \"Feature‐shape mismatch!\"\n",
    "assert torch.allclose(loaded_feats_norm, all_feats_norm), \"Feature‐data mismatch!\"\n",
    "assert loaded_dates.shape == all_dates_tensor.shape, \"Dates‐shape mismatch!\"\n",
    "assert torch.equal(loaded_dates, all_dates_tensor), \"Dates‐data mismatch!\"\n",
    "\n",
    "print(f\"✅ Verified reload for {mode.upper()} normalized features:\")\n",
    "print(f\"   • features_norm shape = {loaded_feats_norm.shape}\")\n",
    "print(f\"   • dates           shape = {loaded_dates.shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
